#--------------------------------------------------------
# 1. Ajuste  de  un  modelo  de  regresi√≥n  lineal  con  R
#--------------------------------------------------------
# Cargamos el dataset de publicidad
Advertising <- read.csv("Advertising.csv")

# Vista previa de los datos
head(Advertising)

# Representaci√≥n gr√°fica de las variables predictoras frente a la variable respuesta (Sales)
pairs(Advertising)

# üìà Interpretaci√≥n del gr√°fico pairs():
# La funci√≥n pairs() genera una matriz de gr√°ficos de dispersi√≥n (scatterplots) para cada
# combinaci√≥n de variables del dataset.

# - Cada celda muestra la relaci√≥n entre dos variables distintas.
# - Los puntos alineados en una diagonal creciente indican correlaci√≥n positiva.
# - Los puntos alineados en una diagonal descendente indican correlaci√≥n negativa.
# - Si los puntos est√°n dispersos sin forma clara, las variables est√°n poco o nada correlacionadas.
# - La diagonal principal de la matriz suele estar vac√≠a o mostrar los nombres de las variables.

# Este tipo de gr√°fico es √∫til para:
# - Detectar relaciones lineales entre variables.
# - Identificar posibles multicolinealidades entre predictores.
# - Observar patrones, agrupaciones o valores at√≠picos.

# Ajustamos el modelo lineal m√∫ltiple con TV, Radio y Newspaper como predictores
z <- lm(Sales ~ TV + Radio + Newspaper, data = Advertising)
class(z)
names(z)
# Vemos el resumen del modelo
summary(z)
# üîç Este resumen incluye:
# - Coeficientes estimados para cada variable
# - Contrastes t individuales (para ver si los coeficientes son significativos)
# - Error est√°ndar residual (RSE)
# - Coeficiente de determinaci√≥n R^2 y su versi√≥n ajustada


# ------------------------------------------------
#1.1 Estimaci√≥n  de  los  par√°metros  del  modelo
# üìå Coeficientes, valores ajustados y residuos

# - coef(z): devuelve los coeficientes estimados del modelo, es decir, los valores Œ≤ÃÇ
#   que mejor ajustan los datos bajo el supuesto de m√≠nimos cuadrados.
#   Incluye el intercepto (Œ≤0) y los coeficientes para cada variable predictora.

# - fitted(z): devuelve los valores ajustados por el modelo, ≈∑ = Œ≤ÃÇ0 + Œ≤ÃÇ1x1 + ... + Œ≤ÃÇpXp,
#   es decir, las predicciones hechas para los datos observados.

# - residuals(z): calcula los residuos ŒµÃÇi = yi - ≈∑i.
#   Muestran la diferencia entre el valor real y el valor predicho por el modelo para cada observaci√≥n.
#   Sirven para evaluar qu√© tan bien se ajusta el modelo a los datos.

# - El RSS (Residual Sum of Squares) se obtiene al sumar los residuos al cuadrado.
#   A partir de este se calcula el RSE (Residual Standard Error), que representa una estimaci√≥n
#   de la desviaci√≥n est√°ndar del t√©rmino de error Œµ.

# - Una menor RSE indica mejor ajuste, aunque debe interpretarse en el contexto de la escala de Y.

# ------------------------------------------------
# Coeficientes estimados
coef(z)

# Valores ajustados por el modelo (≈∑)
fitted(z)

# Residuos del modelo (ŒµÃÇ = y - ≈∑)
residuals(z)

# Verificaci√≥n del c√°lculo del error est√°ndar residual manualmente
RSS <- sum(residuals(z)^2)
n <- nrow(Advertising)
p <- length(coef(z)) - 1
RSE <- sqrt(RSS / (n - p - 1))
RSE

# Intervalos de confianza al 90% para los coeficientes
confint(z, level = 0.9)

#-------------------------------------------------------
#1.2 Contrastes  sobre  los  par√°metros  del  modelo
#-------------------------------------------------------
# El p-valor del F-test (resumen del modelo) indica si al menos una variable es √∫til.
# La variable Newspaper no es significativa (p ‚âà 0.86), as√≠ que ajustamos un nuevo modelo:
z2 <- lm(Sales ~ TV + Radio, data = Advertising)

# Comparamos los R^2
summary(z2)$r.squared      # R^2 sin Newspaper
summary(z)$r.squared       # R^2 con Newspaper
# ‚úÖ Conclusi√≥n:
# El modelo sin Newspaper tiene pr√°cticamente el mismo R^2, por lo tanto Newspaper no aporta
# valor explicativo y puede ser eliminado.


# #---------------------------------------------------------------------------------------------
# 1.3 Prediccion
# - Una vez ajustado el modelo, podemos usarlo para predecir el valor de la variable respuesta
#   (Sales) dados nuevos valores de las variables predictoras (TV y Radio).

# - La funci√≥n predict() permite hacer:
#   a) predicciones puntuales (valor estimado),
#   b) intervalos de confianza para la media de la predicci√≥n,
#   c) intervalos de predicci√≥n para una nueva observaci√≥n.

# - predict(z2, newdata): devuelve la predicci√≥n puntual ≈∑ para los valores en newdata.

# - predict(z2, newdata, interval = "confidence"):
#   devuelve un intervalo que contiene, con determinada probabilidad (por defecto 95%),
#   la media de las respuestas para esas condiciones.

# - predict(z2, newdata, interval = "predict"):
#   devuelve un intervalo m√°s amplio que contiene una posible nueva observaci√≥n de Y
#   para esas condiciones, teniendo en cuenta la variabilidad del error.

#---------------------------------------------------------------------------------------------
# Supongamos que un mercado invierte $100,000 en TV y $20,000 en Radio.

# Creamos el data.frame con estos valores y usamos predict():
newdata <- data.frame(TV = 100, Radio = 20)

# Predicci√≥n puntual
predict(z2, newdata)

# Intervalo de confianza (estimaci√≥n del valor medio esperado)
predict(z2, newdata, interval = "confidence")

# Intervalo de predicci√≥n (valor observado nuevo con error)
predict(z2, newdata, interval = "predict")

#--------------------------------------------------------------
# 2. Ajuste de los par√°metros de un modelo de regresi√≥n
# lineal mediante  m√©todos  de  optimizaci√≥n
#--------------------------------------------------------------

# üé≤ Simulaci√≥n de datos para un modelo de regresi√≥n lineal simple

# - Simulamos un conjunto de datos para observar c√≥mo funciona el ajuste mediante
# descenso de gradiente.
# - El modelo es: y = Œ≤0 + Œ≤1x + Œµ, donde:
#   - x es generado de forma aleatoria (distribuci√≥n uniforme)
#   - Œµ es un t√©rmino de error aleatorio con distribuci√≥n normal
#   - Œ≤0 y Œ≤1 son los par√°metros reales del modelo

# - Estos datos simulan una situaci√≥n en la que conocemos los par√°metros reales,
#   lo que nos permite evaluar la precisi√≥n del ajuste manual.

set.seed(123)  # Fijamos semilla para reproducibilidad
n <- 100
x <- runif(n, min = 0, max = 5)
beta0 <- 2
beta1 <- 5
epsilon <- rnorm(n, sd = 1)
y <- beta0 + beta1 * x + epsilon

# Visualizamos los datos simulados
plot(x, y, main = "Datos simulados para regresion lineal simple", xlab = "x", ylab = "y")

# ‚úÖ Ajuste del modelo mediante m√≠nimos cuadrados con lm()

# - Esta es la soluci√≥n exacta calculada por R utilizando √°lgebra matricial.
# - Nos servir√° como referencia para comparar con los m√©todos iterativos.

modelo_ref <- lm(y ~ x)
coef(modelo_ref)

#---------------------------------------------------------------------------------------
# implementacion del metodo de descenso del gradiente
# - Este metodo consiste en actualizar iterativamente los par√°metros Œ≤0 y Œ≤1
#   para minimizar la funci√≥n de coste J(Œ≤0, Œ≤1), basada en el error cuadr√°tico medio.

# - F√≥rmulas:
#   Œ≤0 ‚Üê Œ≤0 + t * ‚àë(y·µ¢ - (Œ≤0 + Œ≤1 * x·µ¢))
#   Œ≤1 ‚Üê Œ≤1 + t * ‚àë(x·µ¢ * (y·µ¢ - (Œ≤0 + Œ≤1 * x·µ¢)))

# - Donde:
#   - t es la tasa de aprendizaje (learning rate)
#   - El proceso se repite un n√∫mero fijo de iteraciones o hasta converger

# - Es importante elegir bien la tasa de aprendizaje:
#   - Si es muy grande, el algoritmo puede divergir.
#   - Si es muy peque√±a, la convergencia puede ser muy lenta.

#---------------------------------------------------------------------------------------

# Inicializaci√≥n de par√°metros
b0 <- 0
b1 <- 0
t <- 0.001       # Tasa de aprendizaje
iter <- 1000     # N√∫mero de iteraciones

# Iteraciones del descenso de gradiente (batch)
for (i in 1:iter) {
  y_pred <- b0 + b1 * x
  error <- y - y_pred
  b0 <- b0 + t * sum(error)
  b1 <- b1 + t * sum(x * error)
}

# Coeficientes estimados por el metodo batch
c(b0, b1)

# üßæ Comparaci√≥n con el modelo de referencia ajustado con lm()

# - Si el metodo converge correctamente, los coeficientes estimados por gradiente
#   deben estar muy cerca de los obtenidos con lm().
coef(modelo_ref)
#podemos ver que los valores coinciden, lo hemos implementado bien

#------------------------------------------------------------------------------------------------------------
#2.2. Metodo  de  descenso  de  gradiente  estoc√°stico
# ‚öôÔ∏è Descenso de gradiente estoc√°stico (SGD)

# - A diferencia del batch, el SGD actualiza los par√°metros usando **una sola observaci√≥n a la vez**.
# - Esto lo hace mucho m√°s eficiente en datasets grandes, ya que no necesita calcular la suma total
#   de todos los errores en cada iteraci√≥n.

# - Algoritmo:
#   Para cada observaci√≥n i:
#     Œ≤0 ‚Üê Œ≤0 + t * (y·µ¢ - (Œ≤0 + Œ≤1 * x·µ¢))
#     Œ≤1 ‚Üê Œ≤1 + t * x·µ¢ * (y·µ¢ - (Œ≤0 + Œ≤1 * x·µ¢))

# - Aunque SGD es m√°s ruidoso (las actualizaciones var√≠an m√°s), suele converger m√°s r√°pido en la pr√°ctica.
#------------------------------------------------------------------------------------------------------------
# Inicializaci√≥n
b0 <- 0
b1 <- 0
t <- 0.001
epochs <- 10  # N√∫mero de veces que recorremos el dataset entero

# Iteraciones de SGD
for (e in 1:epochs) {
  for (i in 1:n) {
    error_i <- y[i] - (b0 + b1 * x[i])
    b0 <- b0 + t * error_i
    b1 <- b1 + t * x[i] * error_i
  }
}

# Coeficientes estimados por SGD
c(b0, b1)

# Comparar los coeficientes:
coef(modelo_ref)  # Referencia
c(b0, b1)          # SGD estimado

# vemos una vez mas que coinciden, hemos implementado bien
#----------------------------------------------------------------------
# üîç Comparaci√≥n final de m√©todos:

# - lm()     ‚Üí soluci√≥n exacta por m√≠nimos cuadrados
# - batch    ‚Üí soluci√≥n iterativa global, lenta pero precisa
# - SGD      ‚Üí soluci√≥n iterativa r√°pida, eficiente en datos grandes
#----------------------------------------------------------------------

