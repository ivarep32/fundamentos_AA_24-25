#----------------------------------------------------------------
#1. Algoritmo  de  descenso  de  gradiente
# ðŸ“˜ Descenso de gradiente con paso fijo
# Buscamos minimizar la funciÃ³n f(x, y) = 1/2(x^2 + Î“y^2)*
# Su gradiente es: âˆ‡f(x, y) = (x, Î³y)

#* he puesto la gamma mayuscula pa q se diferencie mejor de la y
#-----------------------------------------------------------------

#parametros iniciales
gamma <- 2            # Valor de Î³ (> 0)
x <- c(gamma, 1)      # Punto inicial x(0) = (Î³, 1)^T
eta <- 1e-6           # Criterio de convergencia
max_iter <- 1000      # Iteraciones mÃ¡ximas
t <- 0.1              # Paso fijo (learning rate)
trajectory <- matrix(NA, nrow = max_iter, ncol = 2)  # Guardar trayectoria

# descenso del gradiente
for (k in 1:max_iter) {
  grad <- c(x[1], gamma * x[2])              # Gradiente âˆ‡f(x)
  x_new <- x - t * grad                      # ActualizaciÃ³n: x := x - t * âˆ‡f(x)
  trajectory[k, ] <- x_new                   # Guardamos posiciÃ³n
  if (sqrt(sum((x_new - x)^2)) < eta) break  # Criterio de parada
  x <- x_new
}

trajectory <- trajectory[1:k, ]
x  # SoluciÃ³n final

# ?????????????????????????????????????????????????????????????????????????????????????
# ðŸ“Œ Efecto de t (paso) en la convergencia:
# - Si t es muy pequeÃ±o â†’ convergencia lenta.
# - Si t es demasiado grande â†’ el algoritmo puede divergir.
# - Un valor intermedio adecuado depende de la forma de la funciÃ³n (curvatura = Î³).
# ?????????????????????????????????????????????????????????????????????????????????????

#---------------------------------------------------------------------------------------
#1.1 ElecciÃ³n  del  paso
# ðŸ“˜ Descenso de gradiente con bÃºsqueda exacta del paso (exact line search)
# Queremos minimizar f(x, y) = 1/2 (x^2 + Î³y^2)
# En cada iteraciÃ³n calculamos el paso t que minimiza f(x + s * âˆ†x)
#---------------------------------------------------------------------------------------

# Definimos el valor de Î³
gamma <- 2
# Punto inicial
x <- c(gamma, 1)
# ParÃ¡metros de control
eta <- 1e-6            # Criterio de convergencia
max_iter <- 1000       # Iteraciones mÃ¡ximas
trajectory_exact <- matrix(NA, nrow = max_iter, ncol = 2)

# descenso con busqueda exacta del paso
for (k in 1:max_iter) {
  grad <- c(x[1], gamma * x[2])
  direction <- -grad
  #Paso exacto: t = (âˆ‡f)^T âˆ‡f / (d^T H d)
  # En nuestro caso, H es diagonal: diag(1, Î³^2)
  num <- sum(grad^2)
  denom <- sum(c(1, gamma^2) * direction^2)
  t_exact <- num / denom
  x_new <- x + t_exact * direction
  trajectory_exact[k, ] <- x_new
  if (sqrt(sum((x_new - x)^2)) < eta) break
  x <- x_new
}

# Coordenadas del mÃ­nimo
x

# Pregunta 1: NÃºmero de iteraciones realizadas
cat("Con Î³ = 1, iteraciones necesarias:", k, "\n")

#El metodo converge en 1 iteraciÃ³n cuando ð›¾ =1
# ya que los contornos de la funciÃ³n son circulares y la
# direcciÃ³n del gradiente apunta exactamente hacia el mÃ­nimo.

#Pregunta 2: velocidad de convergencia para distintos valores de Î³
convergencia_por_gamma <- function(gamma) {
  # Definimos las variables necesarias dentro de la funciÃ³n
  max_iter <- 1000
  eta <- 1e-6
  x <- c(gamma, 1)
  
  for (k in 1:max_iter) {
    grad <- c(x[1], gamma * x[2])
    direction <- -grad
    t_exact <- sum(grad^2) / sum(c(1, gamma^2) * direction^2)
    x_new <- x + t_exact * direction
    
    # AÃ±adimos validaciÃ³n para evitar NAs
    if (is.na(sum((x_new - x)^2))) {
      return(max_iter)
    }
    
    if (sqrt(sum((x_new - x)^2)) < eta) break
    x <- x_new
  }
  return(k)
}

# Valores de Î³ a comparar
gammas <- c(0.01, 0.1, 0.5, 1, 2, 5, 10, 100)
iters <- sapply(gammas, convergencia_por_gamma)

# Mostrar resultados
comparacion <- data.frame(gamma = gammas, iteraciones = iters)
print(comparacion)

# ðŸ“Œ AnÃ¡lisis de la velocidad de convergencia:
# - Con Î³ = 1 â†’ convergencia ideal: solo 1 iteraciÃ³n.
# - Cuando 1/3 < Î³ < 3, la convergencia es bastante eficiente.
# - Cuando Î³ â‰ª 1 o Î³ â‰« 1, la funciÃ³n se vuelve muy elÃ­ptica, lo que hace que:
#     - El gradiente apunte en direcciones poco Ãºtiles.
#     - Se necesiten muchas mÃ¡s iteraciones para acercarse al mÃ­nimo.

# Pregunta 3: Programa   el   algoritmo   utilizando   como   paso   de   cada
# iteraciÃ³n   el   obtenido   por   el   metodo backtracking  line  search

# ðŸ“˜ Backtracking Line Search para descenso de gradiente
# Objetivo: minimizar f(x, y) = 1/2(x^2 + Î³ y^2)

# ParÃ¡metros del metodo
gamma <- 2
x <- c(gamma, 1)       # Punto inicial x(0)
eta <- 1e-6            # Criterio de convergencia
max_iter <- 1000       # NÃºmero mÃ¡ximo de iteraciones

alpha <- 0.3           # ParÃ¡metro de control (0, 0.5]
beta <- 0.8            # Factor de reducciÃ³n (0, 1)
trajectory_bt <- matrix(NA, nrow = max_iter, ncol = 2)

# FunciÃ³n objetivo
f <- function(x) {
  0.5 * (x[1]^2 + gamma * x[2]^2)
}

#Gradiente de la funciÃ³n
grad_f <- function(x) {
  c(x[1], gamma * x[2])
}

# Bucle del algoritmo con backtracking
for (k in 1:max_iter) {
  grad <- grad_f(x)
  direction <- -grad         # DirecciÃ³n descendente
  t <- 1                     # Paso inicial

  # Backtracking line search
  while (f(x + t * direction) > f(x) + alpha * t * sum(grad * direction)) {
    t <- beta * t
  }

  x_new <- x + t * direction
  trajectory_bt[k, ] <- x_new

  # Criterio de parada
  if (sqrt(sum((x_new - x)^2)) < eta) {
    cat(sprintf("Convergencia con backtracking en %d iteraciones\n", k))
    x <- x_new
    break
  }

  x <- x_new
}

# SoluciÃ³n final
cat(sprintf(" SoluciÃ³n final: x = (%.6f, %.6f)\n", x[1], x[2]))

# ðŸ“Œ Sobre el metodo de backtracking line search:
# - Se usa cuando no se conoce el paso Ã³ptimo exacto.
# - Permite adaptar dinÃ¡micamente la longitud del paso para asegurar descenso suficiente.
# - Los parÃ¡metros Î± y Î² controlan la "agresividad" del metodo:
#     - Î±: cuÃ¡nto descenso mÃ­nimo se espera (tÃ­picamente entre 0.1 y 0.3)
#     - Î²: cuÃ¡nto se reduce el paso en cada intento (tÃ­picamente entre 0.5 y 0.9)
# - Si Î± es muy pequeÃ±o, se acepta cualquier paso, aunque no sea eficiente.
# - Si Î² es muy pequeÃ±o, se reducen mucho los pasos â†’ lento pero seguro.
# - Puedes probar diferentes combinaciones para ver cÃ³mo afecta la convergencia.

#----------------------------------------------------------------------------------------------------------------------
# 1.2 DemostraciÃ³n  del  algoritmo  de  descenso  de  gradiente
# VisualizaciÃ³n del descenso de gradiente con 'grad.desc':
# - La funciÃ³n grad.desc() genera una animaciÃ³n paso a paso del algoritmo.
# - Muestra cÃ³mo el punto se mueve en la superficie de la funciÃ³n hasta alcanzar el mÃ­nimo.
# - Es Ãºtil para entender cÃ³mo el paso (gamma) afecta a la velocidad y trayectoria de convergencia.
# - init: vector con el punto inicial (x0, y0).
# - gamma: tamaÃ±o del paso (learning rate); si es muy grande, puede rebotar; si es muy pequeÃ±o, avanza lento.

# Puedes experimentar cambiando 'gamma' o el punto inicial para observar diferentes comportamientos.

#----------------------------------------------------------------------------------------------------------------------

# Cargar el paquete 'animation' para visualizaciÃ³n animada
# Si no estÃ¡ instalado, debes ejecutar primero:install.packages("animation")
# a poder ser en los servidores de austria (si Manuel Febrero es tu profesor
# ya sabrÃ¡s porquÃ©)
library(animation)

# Configuramos los parÃ¡metros de la animaciÃ³n
# - interval: tiempo entre fotogramas (en segundos)
# - nmax: nÃºmero mÃ¡ximo de iteraciones que mostrarÃ¡ la animaciÃ³n
ani.options(interval = 0.3, nmax = 50)

# Definimos la funciÃ³n a minimizar
# f(x, y) = 1/2 (x^2 + 2y^2)
fun_obj <- function(x, y) 0.5 * (x^2 + 2 * y^2)

# Ejecutamos la animaciÃ³n del descenso de gradiente
# - FUN: funciÃ³n objetivo
# - init: punto inicial
# - gamma: tasa de aprendizaje (paso fijo)
result <- grad.desc(FUN = fun_obj, init = c(2, 1), gamma = 0.2)

# Imprime la soluciÃ³n final encontrada
result$par

#Visualiza la trayectoria sobre la superficie 3D de la funciÃ³n
result$persp(col = "lightblue", phi = 30)

