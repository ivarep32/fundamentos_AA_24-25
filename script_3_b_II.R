#----------------------------------------------------------------
#1. Algoritmo  de  descenso  de  gradiente
# ðŸ“˜ Descenso de gradiente con paso fijo
# Buscamos minimizar la funciÃ³n f(x, y) = 1/2(x^2 + Î“y^2)*
# Su gradiente es: âˆ‡f(x, y) = (x, Î³y)

#* he puesto la gamma mayuscula pa q se diferencie mejor de la y
#-----------------------------------------------------------------

#parametros iniciales
gamma <- 2            # Valor de Î³ (> 0)
x <- c(gamma, 1)      # Punto inicial x(0) = (Î³, 1)^T
eta <- 1e-6           # Criterio de convergencia
max_iter <- 1000      # Iteraciones mÃ¡ximas
t <- 0.1              # Paso fijo (learning rate)
trajectory <- matrix(NA, nrow = max_iter, ncol = 2)  # Guardar trayectoria

# descenso del gradiente
for (k in 1:max_iter) {
  grad <- c(x[1], gamma * x[2])              # Gradiente âˆ‡f(x)
  x_new <- x - t * grad                      # ActualizaciÃ³n: x := x - t * âˆ‡f(x)
  trajectory[k, ] <- x_new                   # Guardamos posiciÃ³n
  if (sqrt(sum((x_new - x)^2)) < eta) break  # Criterio de parada
  x <- x_new
}

trajectory <- trajectory[1:k, ]
x  # SoluciÃ³n final

# ?????????????????????????????????????????????????????????????????????????????????????
# ðŸ“Œ Efecto de t (paso) en la convergencia:
# - Si t es muy pequeÃ±o â†’ convergencia lenta.
# - Si t es demasiado grande â†’ el algoritmo puede divergir.
# - Un valor intermedio adecuado depende de la forma de la funciÃ³n (curvatura = Î³).
# ?????????????????????????????????????????????????????????????????????????????????????

#---------------------------------------------------------------------------------------
#1.1 ElecciÃ³n  del  paso
# ðŸ“˜ Descenso de gradiente con bÃºsqueda exacta del paso (exact line search)
# Queremos minimizar f(x, y) = 1/2 (x^2 + Î³y^2)
# En cada iteraciÃ³n calculamos el paso t que minimiza f(x + s * âˆ†x)
#---------------------------------------------------------------------------------------

# Definimos el valor de Î³
gamma <- 2

# Punto inicial
x <- c(gamma, 1)

# ParÃ¡metros de control
eta <- 1e-6            # Criterio de convergencia
max_iter <- 1000       # Iteraciones mÃ¡ximas
trajectory_exact <- matrix(NA, nrow = max_iter, ncol = 2)

# descenso con busqueda exacta del paso
for (k in 1:max_iter) {
  grad <- c(x[1], gamma * x[2])
  direction <- -grad

  #Paso exacto: t = (âˆ‡f)^T âˆ‡f / (d^T H d)
  # En nuestro caso, H es diagonal: diag(1, Î³^2)
  num <- sum(grad^2)
  denom <- sum(c(1, gamma^2) * direction^2)
  t_exact <- num / denom

  x_new <- x + t_exact * direction
  trajectory_exact[k, ] <- x_new

  if (sqrt(sum((x_new - x)^2)) < eta) break
  x <- x_new
}
# Coordenadas del mÃ­nimo
x

# Pregunta 1: NÃºmero de iteraciones realizadas
cat("Con Î³ = 1, iteraciones necesarias:", k, "\n")

#El metodo converge en 1 iteraciÃ³n cuando ð›¾ =1
# ya que los contornos de la funciÃ³n son circulares y la
# direcciÃ³n del gradiente apunta exactamente hacia el mÃ­nimo.

#Pregunta 2: velocidad de convergencia para distintos valores de Î³
convergencia_por_gamma <- function(gamma) {
  x <- c(gamma, 1)
  for (k in 1:max_iter) {
    grad <- c(x[1], gamma * x[2])
    direction <- -grad
    t_exact <- sum(grad^2) / sum(c(1, gamma^2) * direction^2)
    x_new <- x + t_exact * direction
    if (sqrt(sum((x_new - x)^2)) < eta) break
    x <- x_new
  }
  return(k)
}

# Valores de Î³ a comparar
gammas <- c(0.01, 0.1, 0.5, 1, 2, 5, 10, 100)
iters <- sapply(gammas, convergencia_por_gamma)

# Mostrar resultados
comparacion <- data.frame(gamma = gammas, iteraciones = iters)
print(comparacion)

