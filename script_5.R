#-----------------------------------------------------------
# REGRESI√ìN LINEAL GENERAL CON R (USANDO savings)
#-----------------------------------------------------------
#-----------------------------------------------------------
# Secci√≥n 1: Ajuste de un modelo de regresi√≥n lineal
# Objetivo: Ajustar un modelo de regresi√≥n lineal m√∫ltiple con R
# sobre la base de datos `savings` del paquete `faraway`.
# El modelo que se propone es:
# Y = Œ≤0 + Œ≤1*pop15 + Œ≤2*pop75 + Œ≤3*dpi + Œ≤4*ddpi + Œµ
# donde Y = tasa de ahorro (sr), y el resto son variables explicativas
# socioecon√≥micas.
#-----------------------------------------------------------
# Cargamos la librer√≠a faraway que contiene los datos
library(faraway)
data("savings")
library(ggplot2)
library(GGally)

ggpairs(savings) + theme_bw()

#Ajuste modelo regrsion lineal multiple
z <- lm(sr ~ pop15 + pop75 + dpi + ddpi, data = savings)
summary(z)

# El resumen proporciona:
# - Coeficientes estimados (Œ≤ÃÇ)
# - Error est√°ndar
# - Valor t y p-valor para cada coeficiente
# - R¬≤ y RSE (residual standard error)

# Interpretaci√≥n:
# Por ejemplo, un coeficiente negativo en pop15 indica que a mayor % de
# poblaci√≥n joven, menor tasa de ahorro, con significaci√≥n estad√≠stica.

#-----------------------------------------------------------
# Secci√≥n 1.1: Estimaci√≥n de los par√°metros del modelo
#-----------------------------------------------------------
beta <- coef(z)

#matriz del dise√±o
X <- model.matrix(z)

# numero de observaciones y parametros
n<-nrow(X)
p <- ncol(X)

#-----------------------------------------------------------
# Secci√≥n 1.2: Interpretaci√≥n geom√©trica
# En regresi√≥n, la predicci√≥n se expresa matricialmente como:
# YÃÇ = H * Y, donde H = X(X'X)^(-1)X' es la matriz hat.
# Esta proyecta Y en el subespacio generado por las columnas de X.

#-----------------------------------------------------------

XtXi <- solve(t(X) %*% X)
H <- X %*% XtXi %*% t(X)

# C√°lculo manual del vector Œ≤ÃÇ
y <- savings$sr
hbeta <- XtXi %*% t(X) %*% y
hbeta  # debe coincidir con coef(modelo)

# Matriz generadora de residuos: M = I - H
M <- diag(1, n) - H

# Residuos estimados: ŒµÃÇ = M * Y
residuos <- M %*% y

# Tambi√©n se pueden obtener con:
residuals(z)

#-----------------------------------------------------------
# Secci√≥n 2: Estimaci√≥n de la varianza
# Para estimar la varianza del error œÉ¬≤ se utiliza:
# œÉÃÇ¬≤ = RSS / (n - p), donde RSS = ‚àë(ŒµÃÇi)¬≤ = ŒµÃÇ'ŒµÃÇ = Y'MY
#-----------------------------------------------------------
# C√°lculo de RSS (Residual Sum of Squares)
RSS <- t(y - X %*% hbeta) %*% (y - X %*% hbeta)

# Equivalente con funci√≥n deviance:
deviance(z)

# Varianza estimada del error
sigma2 <- RSS / (n - p)
sigma2

#-----------------------------------------------------------
# SECCI√ìN 3: Regresi√≥n particionada y parcial (Greene.txt)
# Datos de ejemplo manuales como en el PDF
#-----------------------------------------------------------
# Leer el archivo Greene.txt con separador de tabulaci√≥n y comas como decimales
datos <- read.table("Greene.txt", header = TRUE, sep = "\t", dec = ",")

# Verificamos
str(datos)
datos_real <- datos
names(datos_real) <- c("Anio", "PNB", "IN", "IPC", "TI")

# Convertimos a√±os en √≠ndice de 1 a 15
datos_real$Anio <- 1:15

# Convertimos las variables PNB e IN a d√≥lares constantes del a√±o 1972,
# dividiendo por el IPC y multiplicando por 100.
# Esto elimina el efecto del aumento de precios en los valores monetarios.
# Ajustamos PNB e IN a valores constantes usando IPC
datos_real$PNB <- (datos$PNB / datos$IPC) * 100
datos_real$IN  <- (datos$IN / datos$IPC) * 100

# Usamos la serie IPC para calcular la tasa de inflaci√≥n a√±o a a√±o.
# Para 1968 usamos un IPC externo (79.06 en 1967), luego usamos cocientes sucesivos.
# La tasa de inflaci√≥n se define como (IPC_t / IPC_{t-1}) - 1.
datos_real$IPC[1] <- (datos$IPC[1] / 79.06 - 1) * 100  # Valor base para 1968
datos_real$IPC[2:15] <- (datos$IPC[2:15] / datos$IPC[1:14] - 1) * 100

# Se ajusta un modelo de regresi√≥n simple para estudiar la relaci√≥n
# entre la inversi√≥n (IN) y el a√±o (Anio).
# Este modelo no tiene en cuenta otras variables posibles (como PNB, IPC o TI),
# por lo tanto, puede estar sesgado por omisi√≥n de variables relevantes.
m1 <- lm(IN ~ Anio, datos_real)
summary(m1)

# Ahora ajustamos un modelo m√∫ltiple que incluye como variables explicativas:
# - Anio (a√±o)
# - PNB ajustado por inflaci√≥n
# - IPC (tasa de inflaci√≥n)
# - TI (tipo de inter√©s)
#
# Este modelo controla por otras influencias sobre la inversi√≥n, lo que permite
# interpretar el efecto "neto" o "parcial" de cada variable.
# Aqu√≠, el coeficiente del a√±o podr√≠a cambiar de signo si hay confusi√≥n con PNB.
m2 <- lm(IN ~ Anio + PNB + IPC + TI, datos_real)
summary(m2)

# Separamos el efecto del a√±o sobre la inversi√≥n, controlando por las dem√°s variables.
# Esto se hace en tres pasos:
#
# Paso 1: Ajustar la inversi√≥n (IN) con PNB, IPC y TI --> obtener residuos r_IN
# Paso 2: Ajustar el a√±o (Anio) con PNB, IPC y TI --> obtener residuos r_Anio
# Paso 3: Regresar r_IN sobre r_Anio. La pendiente es el efecto parcial del a√±o.
#
# Esta t√©cnica demuestra que en regresi√≥n m√∫ltiple, el coeficiente de una variable
# se puede interpretar como su efecto "a√±adido" despu√©s de haber descontado los efectos de otras.
#-----------------------------------------------------------

r_IN <- residuals(lm(IN ~ PNB + IPC + TI, datos_real))
r_Anio <- residuals(lm(Anio ~ PNB + IPC + TI, datos_real))

modelo_parcial <- lm(r_IN ~ r_Anio)
summary(modelo_parcial)

# La pendiente de esta regresi√≥n coincide con el coeficiente de Anio en m2

m0 <- lm(IN ~ Anio - 1, datos_real)
summary(m0)

#-----------------------------------------------------------
# SECCI√ìN 4 - Coeficientes de correlaci√≥n simple, m√∫ltiple y parcial
#-----------------------------------------------------------
# Calculamos la matriz de correlaci√≥n simple entre todas las variables del dataset.
# La correlaci√≥n simple entre dos variables mide la intensidad y direcci√≥n de la relaci√≥n lineal.
# El valor est√° en [-1, 1], y su signo coincide con el del coeficiente de regresi√≥n simple.
#-----------------------------------------------------------

correlaciones_simples <- cor(datos_real)
print(correlaciones_simples)

# Podemos extraer la fila o columna correspondiente a IN (Inversi√≥n)
correlaciones_IN <- correlaciones_simples["IN", ]
print(correlaciones_IN)

#-----------------------------------------------------------
#Correlaci√≥n m√∫ltiple
#-----------------------------------------------------------
# El coeficiente de correlaci√≥n m√∫ltiple entre una variable respuesta (IN)
# y un conjunto de variables explicativas (Anio, PNB, IPC, TI) se define como:
#
# Corr(IN, YÃÇ), donde YÃÇ = predicciones del modelo de regresi√≥n m√∫ltiple.
# Su cuadrado coincide con el R¬≤ del modelo de regresi√≥n m√∫ltiple.
#Comentario: Esta medida nos dice qu√© tan bien se ajustan las predicciones
# del modelo a la inversi√≥n observada. Cuanto m√°s cercana a 1, mejor
# explicaci√≥n conjunta.
#-----------------------------------------------------------

correlacion_multiple <- cor(datos_real$IN, fitted(m2))
print(correlacion_multiple)

# Verifica que el cuadrado de este valor es igual a Multiple R-squared:
summary(m2)$r.squared  # Debe coincidir con correlacion_multiple^2

#-----------------------------------------------------------
# Correlaci√≥n parcial
#-----------------------------------------------------------
# Calculamos la correlaci√≥n entre los residuos de IN y cada variable,
# despu√©s de eliminar el efecto de las dem√°s. Esto nos da la influencia neta.
#-----------------------------------------------------------

# Residuos de IN y cada variable, controlando por las otras
r_IN_Anio <- residuals(lm(IN ~ PNB + IPC + TI, data = datos_real))
r_Anio    <- residuals(lm(Anio ~ PNB + IPC + TI, data = datos_real))

r_IN_PNB  <- residuals(lm(IN ~ Anio + IPC + TI, data = datos_real))
r_PNB     <- residuals(lm(PNB ~ Anio + IPC + TI, data = datos_real))

r_IN_IPC  <- residuals(lm(IN ~ Anio + PNB + TI, data = datos_real))
r_IPC     <- residuals(lm(IPC ~ Anio + PNB + TI, data = datos_real))

r_IN_TI   <- residuals(lm(IN ~ Anio + PNB + IPC, data = datos_real))
r_TI      <- residuals(lm(TI ~ Anio + PNB + IPC, data = datos_real))

# Correlaciones parciales entre residuos
correlaciones_parciales <- c(
  Anio = cor(r_IN_Anio, r_Anio),
  PNB  = cor(r_IN_PNB,  r_PNB),
  IPC  = cor(r_IN_IPC,  r_IPC),
  TI   = cor(r_IN_TI,   r_TI)
)
# Resultado esperado: El valor de la correlaci√≥n parcial entre IN y
# Anio debe ser negativo (~ -0.94), a pesar de que su correlaci√≥n
# simple era positiva. Esto evidencia un cambio de signo t√≠pico del
# fen√≥meno de confusi√≥n

#-----------------------------------------------------------
# Tabla resumen: Correlaciones simples vs. parciales
#-----------------------------------------------------------
# Esta tabla permite comparar la relaci√≥n directa y la neta
# de cada variable con la inversi√≥n (IN). El cambio de signo
# indica un posible efecto de confusi√≥n entre variables.
#-----------------------------------------------------------

tabla <- data.frame(
  Correlacion_Simple  = correlaciones_IN[c("Anio", "PNB", "IPC", "TI")],
  Correlacion_Parcial = correlaciones_parciales
)

print(round(tabla, 4))

#-----------------------------------------------------------
# SECCION 5 - Inferencia  sobre  los  par√°metros  con  R
#-----------------------------------------------------------
#Contexto te√≥rico
#-----------------------------------------------------------
# En regresi√≥n lineal m√∫ltiple, se desea construir intervalos de confianza
# para cada par√°metro Œ≤ÃÇ·µ¢ del modelo:
#
#   ( Œ≤ÃÇ·µ¢ ¬± t_{n-p, Œ±/2} ¬∑ œÉÃÇ ¬∑ sqrt[(X'X)^(-1)·µ¢·µ¢] )
#
# Donde:
# - Œ≤ÃÇ·µ¢: estimaci√≥n del par√°metro
# - t_{n-p, Œ±/2}: cuantil de la t de Student con (n - p) grados de libertad
# - œÉÃÇ¬≤: varianza residual estimada
# - (X'X)^(-1): inversa de la matriz de dise√±o
#-----------------------------------------------------------
#-----------------------------------------------------------
# Estimaci√≥n de intervalos de confianza al 95% para los coeficientes
# seg√∫n el modelo ajustado sobre los datos "savings"
#-----------------------------------------------------------

# Recordamos que z es el modelo ajustado:
# z <- lm(sr ~ pop15 + pop75 + dpi + ddpi, data = savings)

# Extraemos la matriz del dise√±o del modelo
X <- model.matrix(z)
n <- nrow(X)
p <- ncol(X)

# Obtenemos la inversa de X'X
XtXi <- solve(t(X) %*% X)

# Estimaci√≥n puntual de los coeficientes
beta <- coef(z)

# Varianza residual (sigma^2)
y <- savings$sr
RSS <- t(y - X %*% beta) %*% (y - X %*% beta)
sigma2 <- RSS / (n - p)

#-----------------------------------------------------------
# C√°lculo de errores t√≠picos de los coeficientes
#-----------------------------------------------------------
ET <- sqrt(rep(sigma2, length(diag(XtXi))) * diag(XtXi))

#-----------------------------------------------------------
# Cuantil t para nivel de confianza 95%
#-----------------------------------------------------------
niv <- 0.95
t <- qt(1 - (1 - niv)/2, n - p)

#-----------------------------------------------------------
# C√°lculo de los intervalos de confianza
#-----------------------------------------------------------

#--- Extremos inferiores de los intervalos de confianza
betainf <- beta - t * ET

#--- Extremos superiores de los intervalos de confianza
betasup <- beta + t * ET

# Mostramos los resultados
betainf
betasup
#-----------------------------------------------------------
# Verificaci√≥n con confint()
#-----------------------------------------------------------
confint(z, level = niv)

#-----------------------------------------------------------------
# SECCION 6 - Descomposici√≥n  de  la  variabilidad.  El  test  F
#-----------------------------------------------------------------
# Objetivo: comparar dos modelos de regresi√≥n anidados
# Modelo completo: sr ~ pop15 + pop75 + dpi + ddpi
# Modelo restringido: sr ~ pop15 + pop75 + dpi  (sin ddpi)
#-----------------------------------------------------------
# Test F para comparar modelos de regresi√≥n anidados
#-----------------------------------------------------------
# Compara un modelo completo y otro restringido (con menos variables).
#
# H0: los coeficientes eliminados son cero (modelo restringido es v√°lido)
# H1: al menos uno de ellos es distinto de cero (modelo completo es mejor)
#
# Estad√≠stico:
#   F = [(RSS_restr - RSS_comp) / q] / [RSS_comp / (n - p)]
#   Donde:
#     - RSS: suma de residuos al cuadrado
#     - q: n¬∫ de restricciones (coeficientes eliminados)
#     - p: n¬∫ de par√°metros del modelo completo
#
# Si el p-valor asociado a F es peque√±o (ej. < 0.05),
# se rechaza H0 y se concluye que las variables eliminadas son significativas.
#
# En R: anova(modelo_restringido, modelo_completo)
#-----------------------------------------------------------


# Modelo restringido: sin la variable ddpi
z1 <- lm(sr ~ pop15 + pop75 + dpi, data = savings)

# Test F usando la funci√≥n anova()
anova(z1, z)

#-------------------------------
# calculo manual del test F
#-------------------------------
# RSS de cada modelo
rss0 <- deviance(z1)
rss <- deviance(z)

# Diferencia de RSS
rss0 - rss

# C√°lculo manual del estad√≠stico F
f <- ((rss0 - rss) / 1) / (rss / (n - p))
pvalue <- 1 - pf(f, 1, n - p)

# Mostrar resultados
f
pvalue

#-----------------------------------------------------
#Segundo contraste (dos variables a la vez)
#Eliminamos simult√°neamente las variables pop75 y dpi
#-----------------------------------------------------
# Modelo completo
z <- lm(sr ~ pop15 + pop75 + dpi + ddpi, data = savings)

# Modelo restringido (eliminamos pop75 y dpi)
z2 <- lm(sr ~ pop15 + ddpi, data = savings)

# C√°lculo manual del test F
rss0 <- deviance(z2)  # RSS del modelo restringido
rss  <- deviance(z)   # RSS del modelo completo

q <- 2           # n¬∫ de restricciones
n <- nrow(model.matrix(z))
p <- ncol(model.matrix(z))

f <- ((rss0 - rss) / q) / (rss / (n - p))
pvalue <- 1 - pf(f, q, n - p)

f
pvalue
anova(z2, z)

#-----------------------------------------------------------
# SECCI√ìN 7: Predicci√≥n con el modelo ajustado
#-----------------------------------------------------------
# Obtenemos las predicciones puntuales para los datos de la muestra
#Aplicando  la  funci√≥n  predict  obtenemos  las  predicciones  de
# la  tasa  de  ahorro  considerando  los valores  observados  en  la  muestra.
#-----------------------------------------------------------
predicciones_muestra <- predict(z)
print(predicciones_muestra)

# Salida:
# Predicciones para los primeros pa√≠ses:
# Australia: 10.57, Austria: 11.45, ..., Malaysia: 7.68
# Esto representa la tasa de ahorro estimada por el modelo para cada observaci√≥n.
#-----------------------------------------------------------

-----------------------------------------------------------
# Predicci√≥n para un nuevo pa√≠s hipot√©tico:
# - pop15 = 30%, pop75 = 2%, dpi = 1000, ddpi = 5
#-----------------------------------------------------------
predict(z, data.frame(pop15 = 30, pop75 = 2, dpi = 1000, ddpi = 5))
# EJECUTAR ESTA LINEA EN LA LINEA DE COMANDOS NO PREGUNTES POR QUE

#-----------------------------------------------------------
# Intervalos de confianza y predicci√≥n
#-----------------------------------------------------------
# Intervalo de confianza para la media condicional (nivel 95%)
predict(z,data.frame(pop15=c(30,40),pop75=c(2,1.5),dpi=c(1000,500), ddpi=c(5,4)),interval="confidence")

# Intervalo de predicci√≥n para una observaci√≥n individual (nivel 95%)
predict(z,data.frame(pop15=c(30,40),pop75=c(2,1.5),dpi=c(1000,500), ddpi=c(5,4)),interval="prediction")

# - El intervalo de confianza (5.06, 21.06) indica que, con un 95% de confianza,
#   la tasa de ahorro media para pa√≠ses con estas caracter√≠sticas est√° en este rango.
# - El intervalo de predicci√≥n (1.28, 16.82) es m√°s amplio, ya que incluye la variabilidad
#   del error aleatorio Œµ.

#-----------------------------------------------------------
# SECCI√ìN 8: Ejercicios propuestos
#-----------------------------------------------------------
# EJERICIO 1
#-----------------------------------------------------------
# Apartado  a
#-----------------------------------------------------------
# Cargar librer√≠a y datos
library(faraway)
data(gala)

# Ajustar modelo (todas las variables excepto "Species" como predictoras)
modelo_gala <- lm(Species ~ ., data = gala)
summary(modelo_gala)

# Intervalos de confianza al 95% para los coeficientes
confint(modelo_gala, level = 0.95)

#Se ajusta un modelo m√∫ltiple usando todas las variables excepto Species como predictoras.
# El resumen muestra los coeficientes estimados y su significaci√≥n.
# Los intervalos de confianza indican el rango plausible para cada par√°metro al 95%

#-----------------------------------------------------------
# Apartado  b
# matriz de dise√±o y matriz hat
#-----------------------------------------------------------
X <- model.matrix(modelo_gala) # matriz de dise√±o
XtXi <- solve(t(X) %*% X)
H <- X %*% XtXi %*% t(X) #matriz hat

#-----------------------------------------------------------
# Apartado  c
# varianza residual e intervalo de cofianza para œÉ^2 al 95 %.
#-----------------------------------------------------------
#Extraer dimensiones del problema (n = muestras, p = par√°metros)
n <- nrow(X)
p <- ncol(X)
#Obtener el vector de respuesta y las predicciones del modelo
y <- gala$Species
beta <- coef(modelo_gala)
#Calcular los residuos (observado - predicho)
residuos <- y - X %*% beta
# Calcular la suma residual de cuadrados (RSS)
RSS <- sum(residuos^2)
#Calcular la varianza residual estimada (œÉ^2)
sigma2 <- RSS / (n - p)

# Calcular un intervalo de confianza al 95% para la varianza œÉ^2
# Utilizamos los cuantiles de la distribuci√≥n Chi-cuadrado
nivel <- 0.95
alfa <- 1 - nivel
IC_inf <- (n - p) * sigma2 / qchisq(1 - alfa/2, df = n - p)
IC_sup <- (n - p) * sigma2 / qchisq(alfa/2, df = n - p)

# Mostrar resultados de la varianza estimada y su intervalo
sigma2
c(inf = IC_inf, sup = IC_sup)

#-----------------------------------------------------------
# Apartado  d
# coeficientes de correlaci√≥n simple y parcial del n√∫mero de
# especies sobre las otras variables
#-----------------------------------------------------------
#Identificamos las variables predictoras
variables <- names(gala)[names(gala) != "Species"]

#Calculamos la correlaci√≥n simple entre Species y cada predictora
cor_simple <- sapply(variables, function(v) cor(gala$Species, gala[[v]]))

# Calcular correlaciones parciales manualmente usando residuos
# Para cada variable Xj, calculamos:
# - Residuos de Species ~ otras variables
# - Residuos de Xj ~ otras variables
# - Correlaci√≥n entre ambos residuos = correlaci√≥n parcial

cor_parcial_manual <- numeric(length(variables))
names(cor_parcial_manual) <- variables

for (var in variables) {
  otras_vars <- setdiff(variables, var)

  # Residuos de Species respecto a las otras variables
  r_species <- residuals(lm(Species ~ ., data = gala[, c("Species", otras_vars)]))

  # Residuos de la variable actual respecto a las otras
  r_x <- residuals(lm(gala[[var]] ~ ., data = gala[, otras_vars]))

  # Correlaci√≥n parcial = correlaci√≥n entre residuos
  cor_parcial_manual[var] <- cor(r_species, r_x)
}
#Mostrar tabla comparativa entre correlaciones simples y parciales
tabla <- data.frame(
  Variable = variables,
  Correlacion_Simple  = round(cor_simple[variables], 4),
  Correlacion_Parcial = round(cor_parcial_manual, 4)
)

print(tabla)

#-----------------------------------------------------------
# Apartado  e
# Aplica  el  F-test  y  comenta  los  resultados
#-----------------------------------------------------------
# Modelo completo (todas las variables)
modelo_full <- lm(Species ~ ., data = gala)

# Modelo restringido (sin Endemics y Area)
#viendo el summary son las de mayor significancia (menor p valor)
modelo_restringido <- lm(Species ~ Elevation + Nearest + Scruz + Adjacent, data = gala)

# C√°lculo manual del test F
rss0 <- deviance(modelo_restringido)  # RSS del modelo restringido
rss  <- deviance(modelo_full)   # RSS del modelo completo

q <- 2           # n¬∫ de restricciones
n <- nrow(model.matrix(modelo_full))
p <- ncol(model.matrix(modelo_full))

f <- ((rss0 - rss) / q) / (rss / (n - p))
pvalue <- 1 - pf(f, q, n - p)

f
pvalue

#hacemos anova
anova(modelo_restringido, modelo_full)

#vamos a probar ahora a quitar la variable menos significativa (Adjacent)
modelo_restringido_2 <- lm(Species ~ Endemics + Area + Elevation + Nearest + Scruz, data = gala)

rss0 <- deviance(modelo_restringido_2)  # RSS del modelo restringido

f_2 <- ((rss0 - rss) / q) / (rss / (n - p))
pvalue_2 <- 1 - pf(f_2, q, n - p)

f_2
pvalue_2

#hacemos anova
anova(modelo_restringido_2, modelo_full)

# Calculamos el estad√≠stico F de forma manual para comparar ambos modelos:
# Un valor de F = 44.2 con un p-valor ‚âà 1.3e-08 indica que eliminar 'Endemics' y 'Area'
# reduce significativamente la calidad del modelo. Estas variables son relevantes.

# La funci√≥n anova(modelo_restringido, modelo_full) confirma este resultado, mostrando que la diferencia
# entre modelos es altamente significativa (p < 0.001). Por tanto, no conviene eliminar esas variables.

# En contraste, construimos un segundo modelo restringido eliminando 'Adjacent', que es la variable

# En este caso, el estad√≠stico F = 0.0116 y el p-valor ‚âà 0.988, lo que indica que eliminar 'Adjacent'
# no afecta significativamente al modelo. Su contribuci√≥n es m√≠nima.

# La anova tambi√©n lo confirma: la diferencia entre modelos al eliminar 'Adjacent' no es significativa.

# En resumen:
# - Eliminar variables relevantes (como 'Endemics') degrada mucho el modelo (alta F, bajo p).
# - Eliminar variables irrelevantes (como 'Adjacent') no afecta el modelo (baja F, alto p).
# Esto valida el enfoque basado en la significancia individual y el test F global para evaluar qu√©
# variables conviene eliminar o mantener.

#-----------------------------------------------------------
# EJERICIO 2
#-----------------------------------------------------------
# Apartado  a
#Ajusta un modelo de regresi√≥n m√∫ltiple que explique la cantidad de grasa
# corporal en funci√≥n de las otras variables. Prueba considerando las tres
# variables como explicativas o subconjuntos de dos variables.
#-----------------------------------------------------------
# Cargar datos
fat <- read.table("Fat.txt", header = TRUE, sep = "", dec=".")
names(fat)

#modelo con las 3 variables
modelo_full <- lm(Fat ~ Triceps + Thigh + Midarm, data = fat)
summary(modelo_full)

# modelos con parejas de variables
modelo_parcial_1 <- lm(Fat ~ Triceps + Thigh, data = fat)
modelo_parcial_2 <- lm(Fat ~ Triceps + Midarm, data = fat)
modelo_parcial_3 <- lm(Fat ~ Thigh + Midarm, data = fat)

#comparaciones
anova(modelo_full, modelo_parcial_1)
anova(modelo_full, modelo_parcial_2)
anova(modelo_full, modelo_parcial_3)

#vemos que todos los modelos parciales tienen un pvalor por encima
#de 0.1 lo que significa que por si mismas no son tan significativas
# aunque si lo son cuando estan juntas

#-----------------------------------------------------------
# Apartado b
#Obt√©n  intervalos  de  conÔ¨Åanza  para  los  par√°metros  del
# modelo  ajustado.
#-----------------------------------------------------------
#un par√°metro es estad√≠sticamente significativo al 95% si su intervalo de confianza no incluye el cero.

#intervalos de 95% para el modelo entero
confint(modelo_full)
#Como todos los intervalos contienen el 0, ninguna de las
# variables es significativa individualmente en el modelo completo.
# Esto refuerza lo que ve√≠amos antes con los valores-p altos: hay colinealidad.

# ahora para cada modelo ajustado
confint(modelo_parcial_1)
# aqui hay dos intervalos que no contienen 0
#Thigh se vuelve significativa cuando no est√° Midarm,
# lo que indica que Midarm y Thigh est√°n compartiendo informaci√≥n.

confint(modelo_parcial_2)
# Triceps y Midarm son significativos aqu√≠, cuando Thigh no est√°.
# cada variable gana fuerza cuando se eliminan otras que est√°n correlacionadas con ella.

confint(modelo_parcial_3)
#Thigh es significativo cuando Triceps no est√°.
#Midarm no lo es en este modelo.

#üîé Conclusi√≥n general
#En el modelo completo, nadie es significativo individualmente porque hay colinealidad:
# las variables est√°n correlacionadas y "se pisan".
#Cuando sacas una de las variables, otras se vuelven significativas.
#Por ejemplo, Triceps y Midarm son significativos cuando se elimina Thigh.
#Esto confirma que las variables se "anulan" mutuamente en el modelo completo.

#-----------------------------------------------------------
# Apartado c
#Calcula  los  coeÔ¨Åcientes  de  correlaci√≥n  simple  y
# parcial  de  la  grasa  corporal  con  las  otras variables.
#-----------------------------------------------------------

#Correlaciones simples
#Son las correlaciones de Pearson entre Fat y cada predictor individual, sin ajustar por las otras variables.
cor(fat) # para ver la tabla completa
cor(fat$Fat, fat[, c("Triceps", "Thigh", "Midarm")]) #para ver solo fat contra las variables

# Resultados:
# Triceps:  0.843 ‚Üí correlaci√≥n fuerte positiva
# Thigh:    0.878 ‚Üí correlaci√≥n muy fuerte positiva
# Midarm:   0.142 ‚Üí correlaci√≥n d√©bil positiva
#
# Interpretaci√≥n:
# Triceps y Thigh est√°n altamente correlacionadas con la grasa corporal cuando se observan por separado.
# Midarm, en cambio, no muestra una relaci√≥n fuerte por s√≠ sola.



#Correlaciones parciales
#Miden la correlaci√≥n entre Fat y otra variable, controlando por las dem√°s. Para esto se necesita el paquete ppcor.
library(ppcor)

#calculo de correlaciones parciales
pcor_result <- pcor(fat)
pcor_result$estimate  # muestra las correlaciones parciales
pcor_result$estimate["Fat", ] # muestra solo las de fat

# Resultados:
# Triceps:  0.338 ‚Üí correlaci√≥n parcial moderada positiva
# Thigh:   -0.267 ‚Üí correlaci√≥n parcial d√©bil negativa
# Midarm:  -0.324 ‚Üí correlaci√≥n parcial moderada negativa
#
# Interpretaci√≥n:
# Triceps mantiene una relaci√≥n positiva con la grasa incluso controlando por las otras variables,
# lo que indica que su efecto es m√°s independiente.
# Thigh, a pesar de su alta correlaci√≥n simple, muestra una relaci√≥n negativa d√©bil cuando se
# controlan las otras variables. Esto sugiere que su efecto estaba parcialmente mediado por Triceps.
# Midarm tiene una correlaci√≥n simple baja, pero su relaci√≥n parcial con la grasa es negativa,
# indicando que podr√≠a introducir ruido en el modelo si se incluye.

# Conclusi√≥n:
# - Triceps parece ser el predictor m√°s confiable.
# - Thigh y Midarm pueden estar aportando colinealidad o redundancia.
# - Esta interpretaci√≥n explica por qu√© en el modelo completo ninguna variable fue significativa por s√≠ sola.

